{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "NNWb8Bmwuh"
      },
      "source": [
        "# Max Todd\n",
        "# Data preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "QLsNNn3CEy"
      },
      "source": [
        "Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "A7C1Hp4RzW"
      },
      "source": [
        "# Load the dataset\n",
        "trainDF = pd.read_csv('./data/original/train.csv', encoding='unicode_escape')\n",
        "testDF = pd.read_csv('./data/original/train.csv', encoding='unicode_escape')\n",
        "\n",
        "# Preview the dataset\n",
        "trainDF.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "       textID  ... Density (P/Km\u00b2)\n0  cb774db0d1  ...              60\n1  549e992a42  ...             105\n2  088c60f138  ...              18\n3  9642c003ef  ...             164\n4  358bd9e861  ...              26\n\n[5 rows x 10 columns]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "yR5WGXhvIZ"
      },
      "source": [
        "# Print dataset information\n",
        "print('Training:')\n",
        "trainDF.info()\n",
        "print('\\n')\n",
        "print(f'Test same schema, length = {len(testDF[list(testDF.columns)[0]])}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27481 entries, 0 to 27480\nData columns (total 10 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   textID            27481 non-null  object \n 1   text              27480 non-null  object \n 2   selected_text     27480 non-null  object \n 3   sentiment         27481 non-null  object \n 4   Time of Tweet     27481 non-null  object \n 5   Age of User       27481 non-null  object \n 6   Country           27481 non-null  object \n 7   Population -2020  27481 non-null  int64  \n 8   Land Area (Km\u00b2)   27481 non-null  float64\n 9   Density (P/Km\u00b2)   27481 non-null  int64  \ndtypes: float64(1), int64(2), object(7)\nmemory usage: 2.1+ MB\n\n\nTest same schema, length = 27481\n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "yaiVTg5eGw"
      },
      "source": [
        "Modify the datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "kPy8CgUia2"
      },
      "source": [
        "def removeStopWords(text):\n",
        "    '''\n",
        "    Remove stop words from the text to reduce bias for them\n",
        "    ex. \"is\", \"the\", \"and\", etc.\n",
        "    '''\n",
        "\n",
        "    # error check - return empty string on non string\n",
        "    if type(text) is not str:\n",
        "        return ''\n",
        "\n",
        "    # remove stop words\n",
        "    noStopWords = ''\n",
        "    words = str(text).split()\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            noStopWords += word + ' '\n",
        "\n",
        "    return noStopWords.strip()\n",
        "\n",
        "\n",
        "def normalizeText(text):\n",
        "    '''\n",
        "    Remove unecessary characters and normalize to reduce variety\n",
        "    as much as possible across different messages\n",
        "    '''\n",
        "    text = re.sub(r'<.*?>', '', str(text))\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenizeText(text):\n",
        "    '''\n",
        "    Tokenize the text\n",
        "    '''\n",
        "    tokens = word_tokenize(str(text))\n",
        "    return tokens"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Vheb64xcfk"
      },
      "source": [
        "def loadDataset(path, textColumnName):\n",
        "    '''\n",
        "    Load the dataset and return a new dataframe containing the\n",
        "    preprocessed text data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "     path of the CSV file to load the dataset from\n",
        "    textColumnName: str\n",
        "     name of the column to perfrom the text preprocessing to\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.Dataframe\n",
        "     dataframe of the loaded data from csv file inputted and preprocessed text\n",
        "    '''\n",
        "\n",
        "    # Load df\n",
        "    df = pd.read_csv(path, encoding='unicode_escape')\n",
        "\n",
        "    # Drop nil / null values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Preprocess the data with NLP\n",
        "    print('Normalizing...')\n",
        "    df['normalizedText'] = df[textColumnName].apply(normalizeText)\n",
        "    print('Removing stop words...')\n",
        "    df['normalizeNoStop'] = df['normalizedText'].apply(removeStopWords)\n",
        "    print('Tokenizing...')\n",
        "    df['tokenized'] = df['normalizeNoStop'].apply(tokenizeText)\n",
        "\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "k2syakQBc1"
      },
      "source": [
        "You can save the dataset obtained from the above function to use for models,\n",
        "or continue from here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "G0VOa28MzF"
      },
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Normalizing...\nRemoving stop words...\nTokenizing...\n"
        }
      ],
      "execution_count": 4
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}